{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. PyTorch Workflow Fundamentals\n",
    "\n",
    "This notebook implements a complete, step-by-step deep learning workflow using PyTorch. We will build a simple linear model to learn the pattern of a straight line (`y = weight * X + bias`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and Setup\n",
    "\n",
    "First, we import the necessary libraries and set up our device-agnostic code to use a GPU if one is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure plots appear inline in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data: Create and Prepare Linear Data\n",
    "\n",
    "We create our data using known parameters (`WEIGHT` and `BIAS`). Then, we split it into training (80%) and testing (20%) sets. Finally, we move our data tensors to the target `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create known parameters for our line equation (y = weight * X + bias)\n",
    "WEIGHT = 0.7\n",
    "BIAS = 0.3\n",
    "\n",
    "# Create input data (X) and corresponding labels (y)\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) # dim=1 ensures correct shape for linear model\n",
    "y = WEIGHT * X + BIAS\n",
    "\n",
    "# Split data into training (80%) and testing (20%) sets\n",
    "train_split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "# Send data to target device (GPU/CPU)\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "print(f\"Training data points: {len(X_train)}\")\n",
    "print(f\"Testing data points: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n",
    "    \"\"\"Plots training data, test data and compares predictions.\"\"\"\n",
    "    # Move data to CPU for use with Matplotlib\n",
    "    train_data_cpu = train_data.cpu().numpy()\n",
    "    train_labels_cpu = train_labels.cpu().numpy()\n",
    "    test_data_cpu = test_data.cpu().numpy()\n",
    "    test_labels_cpu = test_labels.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(\"PyTorch Linear Regression Learning\")\n",
    "    plt.xlabel(\"X Feature\")\n",
    "    plt.ylabel(\"Y Label\")\n",
    "    \n",
    "    # Plot training data in blue\n",
    "    plt.scatter(train_data_cpu, train_labels_cpu, c=\"b\", s=4, label=\"Training Data\")\n",
    "    \n",
    "    # Plot test data in green\n",
    "    plt.scatter(test_data_cpu, test_labels_cpu, c=\"g\", s=4, label=\"Testing Data (True)\")\n",
    "\n",
    "    if predictions is not None:\n",
    "        # Plot the predictions in red\n",
    "        predictions_cpu = predictions.cpu().numpy()\n",
    "        plt.scatter(test_data_cpu, predictions_cpu, c=\"r\", s=8, marker='x', label=\"Predictions (Model Output)\")\n",
    "\n",
    "    plt.legend(prop={\"size\": 14})\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model: The Neural Network Architecture\n",
    "\n",
    "We subclass `nn.Module` to create our custom model. Inside, we use `nn.Linear` which automatically creates the `weight` and `bias` parameters for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass nn.Module\n",
    "class LinearRegressionModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # nn.Linear creates the model's parameters (weights and biases)\n",
    "        self.linear_layer = nn.Linear(in_features=1, out_features=1)\n",
    "    \n",
    "    # The forward method defines the computation\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# Create an instance of the model and move it to the target device\n",
    "torch.manual_seed(42) # Set seed for reproducibility\n",
    "model = LinearRegressionModelV2().to(device) \n",
    "\n",
    "print(\"--- Initial Model Parameters (Random) ---\")\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Function and Optimizer\n",
    "\n",
    "-   **Loss Function (`nn.L1Loss`):** Measures how wrong the model's predictions are (Mean Absolute Error).\n",
    "-   **Optimizer (`torch.optim.SGD`):** Tells the model how to update its parameters to reduce the loss (Stochastic Gradient Descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function measures how wrong the model's predictions are.\n",
    "loss_fn = nn.L1Loss() \n",
    "\n",
    "# The optimizer tells the model how to update its parameters to reduce the loss.\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model.parameters(), # Parameters to optimize\n",
    "    lr=0.01                    # Learning Rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop: The Learning Process\n",
    "\n",
    "This is the core of deep learning. We loop over the data many times (`epochs`) and perform the key steps:\n",
    "1.  **Forward pass:** Get predictions.\n",
    "2.  **Calculate loss:** See how wrong the predictions are.\n",
    "3.  **Zero gradients:** Reset optimizer gradients.\n",
    "4.  **Backpropagation:** Calculate gradients (error signals).\n",
    "5.  **Optimizer step:** Update model parameters to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "print(f\"--- Training for {epochs} Epochs ---\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # 1. Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # 2. Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # 3. Calculate loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 4. Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 5. Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 6. Optimizer step (Gradient Descent)\n",
    "    optimizer.step()\n",
    "\n",
    "    # --- Testing/Evaluation ---\n",
    "    model.eval() \n",
    "    with torch.inference_mode():\n",
    "        test_pred = model(X_test)\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch:4d} | Train Loss: {loss:.4f} | Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, let's check the parameters our model has learned. They should be very close to our original `WEIGHT = 0.7` and `BIAS = 0.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Model Parameters (Learned) ---\")\n",
    "print(model.state_dict())\n",
    "print(f\"Original Parameters: Weight={WEIGHT}, Bias={BIAS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making Predictions (Inference) and Visualization\n",
    "\n",
    "Now we use our trained model to make predictions on the test data and visualize the results. The red 'x' marks should align closely with the green 'Testing Data' points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(X_test)\n",
    "\n",
    "# Plot the predictions\n",
    "plot_predictions(X_train, y_train, X_test, y_test, predictions=y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving and Loading the Model\n",
    "\n",
    "This is crucial for deploying your model. We save the model's *parameters* (the `state_dict`) and then load them into a new, untrained instance of the model to prove it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True) # Create models directory\n",
    "\n",
    "MODEL_NAME = \"linear_regression_model.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# Save the model's state_dict (recommended method)\n",
    "print(f\"Saving model state_dict to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model state_dict\n",
    "\n",
    "# 1. Create a new, untrained instance of the model\n",
    "loaded_model = LinearRegressionModelV2().to(device)\n",
    "print(f\"Loaded model's initial state:\\n{loaded_model.state_dict()}\\n\")\n",
    "\n",
    "# 2. Load the saved parameters\n",
    "loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "print(f\"Loaded model's *trained* state:\\n{loaded_model.state_dict()}\\n\")\n",
    "\n",
    "# 3. Verify predictions from the loaded model match the original model\n",
    "loaded_model.eval()\n",
    "with torch.inference_mode():\n",
    "    loaded_model_preds = loaded_model(X_test)\n",
    "\n",
    "is_close = torch.allclose(y_preds, loaded_model_preds)\n",
    "print(f\"Predictions from saved/loaded model match original predictions: {is_close}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
